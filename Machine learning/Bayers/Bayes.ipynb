{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建词典\n",
    "def createVocabList(dataset):\n",
    "    vocabList=set([])\n",
    "    for line in dataset:\n",
    "        vocabList=vocabList|set(line)\n",
    "    return list(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将文本转换成向量\n",
    "def Words2Vec(dataset,vocablist):\n",
    "    Matrix=[]\n",
    "    if type(dataset[0]).__name__=='list':        \n",
    "        txtnum=len(dataset)\n",
    "        for i in range(txtnum):\n",
    "            Vec=[0]*len(vocablist)\n",
    "            for word in dataset[i]:\n",
    "                if word in vocablist:\n",
    "                    Vec[vocablist.index(word)]=1           \n",
    "            Matrix.append(Vec)\n",
    "    else:\n",
    "        Vec=[0]*len(vocablist)\n",
    "        for word in dataset:\n",
    "            if word in vocablist:\n",
    "                Vec[vocablist.index(word)]=1\n",
    "        Matrix=Vec\n",
    "    return Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n"
     ]
    }
   ],
   "source": [
    "vocablist=createVocabList(postingList)\n",
    "print vocablist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "wordsMatrix=Words2Vec(postingList,vocablist)\n",
    "print wordsMatrix\n",
    "print len(wordsMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#朴素贝叶斯分类器\n",
    "def NaiveBayes(Matrix,classList):\n",
    "    sampleNum=len(Matrix)\n",
    "    wordNum=len(Matrix[0])\n",
    "    #计算p(c)\n",
    "    pc0=classList.count(0)/float(len(classList))\n",
    "    pc1=classList.count(1)/float(len(classList))\n",
    "    #计算p(w|c)\n",
    "    pwc0=np.ones((1,wordNum))\n",
    "    pwc1=np.ones((1,wordNum))\n",
    "    ep=2.0\n",
    "    for row in range(len(Matrix)):\n",
    "        if classList[row]==0:\n",
    "            pwc0=pwc0+Matrix[row]\n",
    "        if classList[row]==1:\n",
    "            pwc1=pwc1+Matrix[row]\n",
    "    pwc0=np.log(pwc0/(ep+np.sum(pwc0)))\n",
    "    pwc1=np.log(pwc1/(ep+np.sum(pwc1)))\n",
    "    return pwc0,pwc1,pc0,pc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "pwc0,pwc1,pc0,pc1=NaiveBayes(wordsMatrix,classVec)\n",
    "print pc0,pc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#朴素贝叶斯分类函数\n",
    "def clssifyNB(inputVec,pwc0,pwc1,pc0,pc1):\n",
    "    p0=np.sum(np.multiply(inputVec,pwc0))+pc0\n",
    "    p1=np.sum(np.multiply(inputVec,pwc1))+pc1\n",
    "    return 0 if p0>p1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#测试贝叶斯分类器\n",
    "\n",
    "def predictNB(wordList,vocablist,pwc0,pwc1,pc0,pc1):\n",
    "    wordsVec=np.array(Words2Vec(wordList,vocablist))\n",
    "    result=clssifyNB(wordsVec,pwc0,pwc1,pc0,pc1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stupid', 'garbage', 'love']属于1类！\n"
     ]
    }
   ],
   "source": [
    "wordList=['stupid', 'garbage','love']\n",
    "print str(wordList)+ u'属于'+ str(predictNB(wordList,vocablist,pwc0,pwc1,pc0,pc1))+u'类！'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将文本转换成词袋\n",
    "def bagofWords2Vec(dataset,vocablist):\n",
    "    txtnum=len(dataset)\n",
    "    Matrix=[]\n",
    "    for i in range(txtnum):\n",
    "        Vec=[0]*len(vocablist)\n",
    "        for word in dataset[i]:\n",
    "            if word in vocablist:\n",
    "                Vec[vocablist.index(word)]+=1           \n",
    "        Matrix.append(Vec)\n",
    "    return Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "import jieba\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "def read_tingyongci():\n",
    "    path = \"123.txt\"   #停用词分析\n",
    "    stop_words = []\n",
    "    csv_reader = codecs.open(path,\"r\", \"gbk\")\n",
    "    for row in csv_reader:\n",
    "        row = row.strip( '\\r\\n' ).encode(\"utf-8\")\n",
    "        stop_words.append(row)\n",
    "    return stop_words\n",
    "stop_words = read_tingyongci()\n",
    "\n",
    "# 将utf8的列表转换成unicode\n",
    "def changeListCode(b):\n",
    "    a = []\n",
    "    for i in b:\n",
    "            try:\n",
    "                i1 = i.decode('GBK').encode('utf-8')\n",
    "                a.append(i.decode('GBK').encode('utf-8'))\n",
    "            except:\n",
    "                a.append(i)\n",
    "    return a\n",
    "\n",
    "def str2num(a):\n",
    "    b=[]\n",
    "    for item in a:\n",
    "        b.append(int(item))\n",
    "    return b\n",
    "\n",
    "def readtrain():\n",
    "    with open('train.csv', 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        column1 = [row for row in reader]\n",
    "    content_train = [i[0] for i in column1[0:]] #第一列为文本内容，并去除列名\n",
    "    content_train = changeListCode(content_train)\n",
    "    opinion_train = [i[1] for i in column1[0:]] #第二列为类别，并去除列名\n",
    "    print '训练集有 %s 条句子' % len(content_train)\n",
    "    train = [content_train, opinion_train]\n",
    "    return train\n",
    "\n",
    "def readexam():\n",
    "    with open('exam.csv', 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        column1 = [row for row in reader]\n",
    "    content_exam = [i[0] for i in column1[0:]] #第一列为文本内容，并去除列名\n",
    "    print '测试集有 %s 条句子' % len(content_exam)\n",
    "    return content_exam\n",
    "\n",
    "# 对列表进行分词并用空格连接\n",
    "def segmentWord(cont):\n",
    "    c = []\n",
    "    patt1 = re.compile(r'\\[.*?\\]')\n",
    "    for i in cont:\n",
    "        if i != '':\n",
    "            s=[]\n",
    "            line1 = re.sub(patt1, '', i)\n",
    "            wordList = jieba.cut(line1)\n",
    "            for i in wordList:\n",
    "                if i >= u'\\u4e00' and i <= u'\\u9fa5':\n",
    "                    if i.encode('utf8') in stop_words:\n",
    "                        pass\n",
    "                    else:\n",
    "                        s.append(i.encode('utf8'))\n",
    "            b = \" \".join(s)\n",
    "            c.append(b)\n",
    "    return c\n",
    "\n",
    "#将文档转换成词列表\n",
    "def words2list(words):\n",
    "    wordsList=[]\n",
    "    for line in words:\n",
    "        wordsList.append(line.split(' '))\n",
    "    return wordsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集有 2000 条句子\n",
      "测试集有 171019 条句子\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache c:\\users\\dsh\\appdata\\local\\temp\\jieba.cache\n",
      "Loading model cost 0.310 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d398cd5ff549>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords2list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mopinion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr2num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mexam_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegmentWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mexam_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords2list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexam_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mexam_option\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexam_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-f7298f59b7f7>\u001b[0m in \u001b[0;36msegmentWord\u001b[1;34m(cont)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mline1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mwordList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;34mu'\\u4e00'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;34mu'\\u9fa5'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\jieba\\__init__.pyc\u001b[0m in \u001b[0;36mcut\u001b[1;34m(self, sentence, cut_all, HMM)\u001b[0m\n\u001b[0;32m    304\u001b[0m                 \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre_skip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mre_skip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcut_all\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stopwords = read_tingyongci()\n",
    "train = readtrain()\n",
    "data_exam = readexam()\n",
    "exam = changeListCode(data_exam)\n",
    "content = segmentWord(train[0])\n",
    "content=words2list(content)\n",
    "opinion = str2num(train[1])\n",
    "exam_content = segmentWord(exam)\n",
    "exam_content=words2list(exam_content)\n",
    "exam_option=[0]*len(exam_content)\n",
    "vocablist=createVocabList(content)\n",
    "trainMatrix=Words2Vec(content,vocablist)\n",
    "pwc0,pwc1,pc0,pc1=NaiveBayes(trainMatrix,opinion)\n",
    "for i in range(len(exam_content)):\n",
    "    if exam_content[i]=='':\n",
    "        exam_option[i]=0\n",
    "        continue\n",
    "    exam_option[i]=predictNB(exam_content[i],vocablist,pwc0,pwc1,pc0,pc1)\n",
    "csvfile = file('result_nb.csv', 'wb')\n",
    "writer = csv.writer(csvfile)\n",
    "for i in range(0,exam.__len__()):\n",
    "    writer.writerow([exam_option[i],data_exam[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
